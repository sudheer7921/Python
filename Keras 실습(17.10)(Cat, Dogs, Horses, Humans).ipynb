{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial on CNN implementation for own data set in keras(Anuj shah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,cv2\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'dogs', 'horses', 'Humans']\n"
     ]
    }
   ],
   "source": [
    "#PATH = os.getcwd() #C:\\\\Users\\\\v\\\\\n",
    "# Define data path\n",
    "#data_path = PATH + '/own_data_cnn_implementation_keras.git/trunk/data'\n",
    "data_path = 'C:/Users/v/OneDrive/own_data_cnn_implementation_keras-master/data'\n",
    "\n",
    "data_dir_list = os.listdir(data_path) #data안의 list 들을 읽는 듯 (4개)\n",
    "\n",
    "print(data_dir_list) \n",
    "\n",
    "\n",
    "#for dataset in data_dir_list:\n",
    "#    try:\n",
    "#        img_list=os.listdir(data_path+'/'+dataset)\n",
    "#        print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "#    except NotADirectoryError:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows=128\n",
    "img_cols=128\n",
    "num_channel=1 # 흑백\n",
    "num_epoch=20\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 4\n",
    "\n",
    "img_data_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset cats\n",
      "\n",
      "Loaded the images of dataset dogs\n",
      "\n",
      "Loaded the images of dataset horses\n",
      "\n",
      "Loaded the images of dataset Humans\n",
      "\n",
      "[[194 192 198 ..., 139 138 137]\n",
      " [194 192 198 ..., 140 139 138]\n",
      " [195 192 198 ..., 142 140 140]\n",
      " ..., \n",
      " [220 220 221 ..., 171 171 171]\n",
      " [220 220 221 ..., 172 172 172]\n",
      " [220 220 221 ..., 173 173 173]]\n"
     ]
    }
   ],
   "source": [
    "for dataset in data_dir_list: #리스트의 원소가 문자열 이라도 for문 사용이 가능 하다 \n",
    "    img_list=os.listdir(data_path+'/'+dataset) #낱개 그림 파일들\n",
    "    #print(img_list)\n",
    "    print ('Loaded the images of dataset '+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY) #파이썬은 BGR\n",
    "        input_img_resize=cv2.resize(input_img,(128,128))\n",
    "        img_data_list.append(input_img_resize) #202 * 4개  \n",
    "        \n",
    "print(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 43  44  43 ..., 164 203 193]\n",
      "  [ 42  41  41 ..., 159 190 195]\n",
      "  [ 40  36  35 ..., 159 183 193]\n",
      "  ..., \n",
      "  [ 21  18  18 ...,  43  74  28]\n",
      "  [ 27  21  21 ...,  39  37  35]\n",
      "  [ 29  20  20 ...,  52  40  37]]\n",
      "\n",
      " [[ 35  16  32 ..., 153 152 148]\n",
      "  [ 45  45  48 ..., 153 154 149]\n",
      "  [ 52  57  55 ..., 154 156 151]\n",
      "  ..., \n",
      "  [168 159 150 ..., 169 171 155]\n",
      "  [163 156 156 ..., 171 169 150]\n",
      "  [144 134 137 ..., 171 168 151]]\n",
      "\n",
      " [[222 223 223 ..., 245 235 247]\n",
      "  [219 221 223 ..., 240 237 237]\n",
      "  [224 223 220 ..., 243 243 242]\n",
      "  ..., \n",
      "  [205 207 204 ..., 212 212 213]\n",
      "  [213 210 206 ..., 212 212 213]\n",
      "  [217 210 200 ..., 213 212 213]]\n",
      "\n",
      " ..., \n",
      " [[224 225 226 ..., 211 210 210]\n",
      "  [220 220 221 ..., 207 207 206]\n",
      "  [214 214 215 ..., 202 202 202]\n",
      "  ..., \n",
      "  [ 51  70  54 ...,  56  86  63]\n",
      "  [ 56  67  46 ...,  61  76  60]\n",
      "  [ 49  42  79 ..., 100  69  64]]\n",
      "\n",
      " [[ 20  17  20 ...,  40  41  48]\n",
      "  [ 24  15  33 ...,  59  61  43]\n",
      "  [ 35  14  17 ...,  27  30  38]\n",
      "  ..., \n",
      "  [ 98  99  91 ..., 108 120  87]\n",
      "  [ 87  97 116 ..., 128 118 110]\n",
      "  [109 111 105 ..., 118 123 114]]\n",
      "\n",
      " [[194 194 194 ..., 146 138 137]\n",
      "  [194 194 194 ..., 148 140 139]\n",
      "  [195 195 195 ..., 150 143 141]\n",
      "  ..., \n",
      "  [220 220 221 ..., 170 170 170]\n",
      "  [220 220 221 ..., 172 172 172]\n",
      "  [220 220 221 ..., 173 173 173]]]\n",
      "(808, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "img_data = np.array(img_data_list) #리스트를 배열 형태로 바꾼다.(?)\n",
    "print(img_data)\n",
    "img_data = img_data.astype('float32') #형 변환 연산자\n",
    "#img_data /= 255 # normalization\n",
    "\n",
    "\n",
    "print (img_data.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras와 Tensorflow의 차이는 Image dimension ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808, 1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "if num_channel==1: #흑백\n",
    "\tif K.image_dim_ordering()=='th': #theano\n",
    "\t\timg_data= np.expand_dims(img_data, axis=1) #1차원임을 표시\n",
    "\t\tprint (img_data.shape)\n",
    "\telse:\n",
    "\t\timg_data= np.expand_dims(img_data, axis=4) \n",
    "\t\tprint (img_data.shape)\n",
    "\t\t\n",
    "else:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\timg_data=np.rollaxis(img_data,3,1)\n",
    "\t\tprint (img_data.shape)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUSE_SKLEARN_PREPROCESSING=False\\n\\nif USE_SKLEARN_PREPROCESSING:\\n\\t# using sklearn for preprocessing\\n\\tfrom sklearn import preprocessing\\n\\t\\n\\tdef image_to_feature_vector(image, size=(128, 128)):\\n\\t\\t# resize the image to a fixed size, then flatten the image into\\n\\t\\t# a list of raw pixel intensities\\n\\t\\treturn cv2.resize(image, size).flatten()\\n\\t\\n\\timg_data_list=[]\\n\\tfor dataset in data_dir_list:\\n\\t\\timg_list=os.listdir(data_path+'/'+ dataset)\\n\\t\\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\\n\\t\\tfor img in img_list:\\n\\t\\t\\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\\n\\t\\t\\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\\n\\t\\t\\tinput_img_flatten=image_to_feature_vector(input_img,(128,128))\\n\\t\\t\\timg_data_list.append(input_img_flatten)\\n\\t\\n\\timg_data = np.array(img_data_list)\\n\\timg_data = img_data.astype('float32')\\n\\tprint (img_data.shape)\\n    \\n    #zero mean and unit variance of data(flatten the data)\\n\\timg_data_scaled = preprocessing.scale(img_data)\\n\\tprint (img_data_scaled.shape) #(808, 16384)\\n\\t\\n\\tprint (np.mean(img_data_scaled))\\n\\tprint (np.std(img_data_scaled))\\n\\t\\n\\tprint (img_data_scaled.mean(axis=0))\\n\\tprint (img_data_scaled.std(axis=0))\\n\\t\\n\\tif K.image_dim_ordering()=='th':\\n\\t\\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\\n\\t\\tprint (img_data_scaled.shape)\\n\\t\\t\\n\\telse:\\n\\t\\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\\n\\t\\tprint (img_data_scaled.shape)\\n\\t\\n\\t\\n\\tif K.image_dim_ordering()=='th':\\n\\t\\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\\n\\t\\tprint (img_data_scaled.shape)\\n\\t\\t\\n\\telse:\\n\\t\\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\\n\\t\\tprint (img_data_scaled.shape)\\n\\nif USE_SKLEARN_PREPROCESSING:\\n\\timg_data=img_data_scaled\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "USE_SKLEARN_PREPROCESSING=False\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "\t# using sklearn for preprocessing\n",
    "\tfrom sklearn import preprocessing\n",
    "\t\n",
    "\tdef image_to_feature_vector(image, size=(128, 128)):\n",
    "\t\t# resize the image to a fixed size, then flatten the image into\n",
    "\t\t# a list of raw pixel intensities\n",
    "\t\treturn cv2.resize(image, size).flatten()\n",
    "\t\n",
    "\timg_data_list=[]\n",
    "\tfor dataset in data_dir_list:\n",
    "\t\timg_list=os.listdir(data_path+'/'+ dataset)\n",
    "\t\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "\t\tfor img in img_list:\n",
    "\t\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "\t\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\tinput_img_flatten=image_to_feature_vector(input_img,(128,128))\n",
    "\t\t\timg_data_list.append(input_img_flatten)\n",
    "\t\n",
    "\timg_data = np.array(img_data_list)\n",
    "\timg_data = img_data.astype('float32')\n",
    "\tprint (img_data.shape)\n",
    "    \n",
    "    #zero mean and unit variance of data(flatten the data)\n",
    "\timg_data_scaled = preprocessing.scale(img_data)\n",
    "\tprint (img_data_scaled.shape) #(808, 16384)\n",
    "\t\n",
    "\tprint (np.mean(img_data_scaled))\n",
    "\tprint (np.std(img_data_scaled))\n",
    "\t\n",
    "\tprint (img_data_scaled.mean(axis=0))\n",
    "\tprint (img_data_scaled.std(axis=0))\n",
    "\t\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "\t\tprint (img_data_scaled.shape)\n",
    "\t\t\n",
    "\telse:\n",
    "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "\t\tprint (img_data_scaled.shape)\n",
    "\t\n",
    "\t\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "\t\tprint (img_data_scaled.shape)\n",
    "\t\t\n",
    "\telse:\n",
    "\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "\t\tprint (img_data_scaled.shape)\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "\timg_data=img_data_scaled\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "num_classes = 4\n",
    "\n",
    "print (img_data.shape)\n",
    "\n",
    "num_of_samples = img_data.shape[0] # 808\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "labels[0:202]=0\n",
    "labels[202:404]=1\n",
    "labels[404:606]=2\n",
    "labels[606:]=3\n",
    "\n",
    "names = ['cats','dogs','horses','humans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# convert class labels to on-hot encoding\n",
    "Y = np_utils.to_categorical(labels, num_classes) #★★★\n",
    "print(Y) # 808개 [1,0,0,0] ~ [0,0,0,1]\n",
    "#[0,0,0,0] ~ [0,0,0,3] 이 될줄 알았는데 그건 아니었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "x,y = shuffle(img_data,Y, random_state=2) #808개의 순서를 Shuffle\n",
    "\n",
    "# Split the dataset\n",
    "#1/5로 Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(1, 128, 1..., padding=\"same\")`\n",
      "  \n",
      "c:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n",
      "c:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "input_shape=img_data[0].shape\n",
    "\t\t\t\t\t\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, 3,3,border_mode='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Convolution2D(64, 3, 3))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 128, 128)      320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 128, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 126, 126)      9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 126, 126)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 63, 63)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 63, 63)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 61, 61)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 61, 61)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 30, 30)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 30, 30)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                3686464   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 3,714,788\n",
      "Trainable params: 3,714,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "model.get_config()\n",
    "model.layers[0].get_config()\n",
    "model.layers[0].input_shape\n",
    "model.layers[0].output_shape\n",
    "model.layers[0].get_weights()\n",
    "np.shape(model.layers[0].get_weights()[0])\n",
    "model.layers[0].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 646 samples, validate on 162 samples\n",
      "Epoch 1/20\n",
      "646/646 [==============================] - 146s - loss: 1.6668 - acc: 0.2848 - val_loss: 1.4120 - val_acc: 0.2963\n",
      "Epoch 2/20\n",
      "646/646 [==============================] - 157s - loss: 1.3570 - acc: 0.3607 - val_loss: 1.3416 - val_acc: 0.4383\n",
      "Epoch 3/20\n",
      "646/646 [==============================] - 154s - loss: 1.2158 - acc: 0.4628 - val_loss: 1.2798 - val_acc: 0.4321\n",
      "Epoch 4/20\n",
      "646/646 [==============================] - 157s - loss: 1.0534 - acc: 0.5480 - val_loss: 1.2399 - val_acc: 0.4568\n",
      "Epoch 5/20\n",
      "646/646 [==============================] - 139s - loss: 0.9416 - acc: 0.6146 - val_loss: 1.1616 - val_acc: 0.4630\n",
      "Epoch 6/20\n",
      "646/646 [==============================] - 163s - loss: 0.7854 - acc: 0.6842 - val_loss: 1.0920 - val_acc: 0.5432\n",
      "Epoch 7/20\n",
      "646/646 [==============================] - 129s - loss: 0.6647 - acc: 0.7399 - val_loss: 1.2357 - val_acc: 0.4877\n",
      "Epoch 8/20\n",
      "646/646 [==============================] - 143s - loss: 0.5736 - acc: 0.7755 - val_loss: 1.3186 - val_acc: 0.4938\n",
      "Epoch 9/20\n",
      "646/646 [==============================] - 145s - loss: 0.5088 - acc: 0.8158 - val_loss: 1.2715 - val_acc: 0.5185\n",
      "Epoch 10/20\n",
      "646/646 [==============================] - 157s - loss: 0.4103 - acc: 0.8467 - val_loss: 1.3885 - val_acc: 0.5247\n",
      "Epoch 11/20\n",
      "646/646 [==============================] - 148s - loss: 0.3708 - acc: 0.8638 - val_loss: 1.4415 - val_acc: 0.5309\n",
      "Epoch 12/20\n",
      "646/646 [==============================] - 151s - loss: 0.2601 - acc: 0.9102 - val_loss: 1.7318 - val_acc: 0.5123\n",
      "Epoch 13/20\n",
      "646/646 [==============================] - 158s - loss: 0.2955 - acc: 0.8901 - val_loss: 2.0032 - val_acc: 0.4938\n",
      "Epoch 14/20\n",
      "646/646 [==============================] - 168s - loss: 0.2386 - acc: 0.9133 - val_loss: 1.9073 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "646/646 [==============================] - 142s - loss: 0.1866 - acc: 0.9365 - val_loss: 1.8803 - val_acc: 0.4815\n",
      "Epoch 16/20\n",
      "646/646 [==============================] - 147s - loss: 0.1768 - acc: 0.9458 - val_loss: 1.9720 - val_acc: 0.5309\n",
      "Epoch 17/20\n",
      "646/646 [==============================] - 162s - loss: 0.1645 - acc: 0.9443 - val_loss: 2.1028 - val_acc: 0.5309\n",
      "Epoch 18/20\n",
      "646/646 [==============================] - 151s - loss: 0.1437 - acc: 0.9567 - val_loss: 1.9269 - val_acc: 0.5185\n",
      "Epoch 19/20\n",
      "646/646 [==============================] - 145s - loss: 0.1497 - acc: 0.9551 - val_loss: 2.0446 - val_acc: 0.5370\n",
      "Epoch 20/20\n",
      "646/646 [==============================] - 151s - loss: 0.1182 - acc: 0.9613 - val_loss: 2.1565 - val_acc: 0.5247\n",
      "3023.5751802921295\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "T1 = time.time()\n",
    "hist = model.fit(X_train, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "T2 = time.time()\n",
    "print(T2-T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 646 samples, validate on 162 samples\n",
      "Epoch 1/20\n",
      "640/646 [============================>.] - ETA: 1s - loss: 0.0920 - acc: 0.9703Epoch 00000: val_loss improved from inf to 2.67929, saving model to Best-weights-my_model-000-0.0911-0.9706.hdf5\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-affad776607a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcsv_log\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1201\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    415\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   2551\u001b[0m         \"\"\"\n\u001b[0;32m   2552\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2553\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\v\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`save_model` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_json_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: `save_model` requires h5py."
     ]
    }
   ],
   "source": [
    "# Training with callbacks\n",
    "from keras import callbacks\n",
    "\n",
    "filename='model_train_new.csv'\n",
    "csv_log=callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "\n",
    "early_stopping=callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')\n",
    "\n",
    "filepath=\"Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5\"\n",
    "\n",
    "# 가장 작은 것을 저장.\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [csv_log,early_stopping,checkpoint]\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test),callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping : Stop training when a monitores quantity has stop improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing losses and accuracy\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "xc=range(num_epoch)\n",
    "\n",
    "plt.figure(1,figsize=(7,5))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'])\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "plt.figure(2,figsize=(7,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,val_acc)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'],loc=4)\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "#score = model.evaluate(X_test, y_test, show_accuracy=True, verbose=0)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "print (test_image.shape)\n",
    "\n",
    "print(model.predict(test_image))\n",
    "print(model.predict_classes(test_image))\n",
    "print(y_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a new image\n",
    "test_image = cv2.imread('data/Humans/rider-8.jpg')\n",
    "test_image=cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "test_image=cv2.resize(test_image,(128,128))\n",
    "test_image = np.array(test_image)\n",
    "test_image = test_image.astype('float32')\n",
    "test_image /= 255\n",
    "print (test_image.shape)\n",
    "   \n",
    "if num_channel==1:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\telse:\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=3) \n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\t\t\n",
    "else:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\ttest_image=np.rollaxis(test_image,2,0)\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\telse:\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\t\t\n",
    "        \n",
    "# Predicting the test image\n",
    "print((model.predict(test_image)))\n",
    "print(model.predict_classes(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the intermediate layer\n",
    "def get_featuremaps(model, layer_idx, X_batch):\n",
    "\tget_activations = K.function([model.layers[0].input, K.learning_phase()],[model.layers[layer_idx].output,])\n",
    "\tactivations = get_activations([X_batch,0])\n",
    "\treturn activations\n",
    "\n",
    "layer_num=3\n",
    "filter_num=0\n",
    "\n",
    "activations = get_featuremaps(model, int(layer_num),test_image)\n",
    "\n",
    "print (np.shape(activations)) #(1, 1, 32, 128, 128) <-theano\n",
    "feature_maps = activations[0][0]      \n",
    "print (np.shape(feature_maps))\n",
    "\n",
    "if K.image_dim_ordering()=='th':\n",
    "\tfeature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\n",
    "print (feature_maps.shape)\n",
    "\n",
    "fig=plt.figure(figsize=(16,16))\n",
    "plt.imshow(feature_maps[:,:,filter_num],cmap='gray')\n",
    "plt.savefig(\"featuremaps-layer-{}\".format(layer_num) + \"-filternum-{}\".format(filter_num)+'.jpg')\n",
    "\n",
    "num_of_featuremaps=feature_maps.shape[2]\n",
    "fig=plt.figure(figsize=(16,16))\t\n",
    "plt.title(\"featuremaps-layer-{}\".format(layer_num))\n",
    "subplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\n",
    "for i in range(int(num_of_featuremaps)):\n",
    "\tax = fig.add_subplot(subplot_num, subplot_num, i+1)\n",
    "\t#ax.imshow(output_image[0,:,:,i],interpolation='nearest' ) #to see the first filter\n",
    "\tax.imshow(feature_maps[:,:,i],cmap='gray')\n",
    "\tplt.xticks([])\n",
    "\tplt.yticks([])\n",
    "\tplt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"featuremaps-layer-{}\".format(layer_num) + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(y_pred)\n",
    "#y_pred = model.predict_classes(X_test)\n",
    "#print(y_pred)\n",
    "target_names = ['class 0(cats)', 'class 1(Dogs)', 'class 2(Horses)','class 3(Humans)']\n",
    "\t\t\t\t\t\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix')\n",
    "#plt.figure()\n",
    "# Plot normalized confusion matrix\n",
    "#plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "#plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading model and weights\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.save('model.hdf5')\n",
    "loaded_model=load_model('model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
